{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*David Schlangen, 2019-03-24*\n",
    "\n",
    "# Task: Resolving Co-Reference / Predicting Model Size\n",
    "\n",
    "In the section on discourses in the denotations section, we have already briefly mentioned the task of co-reference resolution. If the image (model) is available, co-reference is established exophorically, via the anchoring in the image object. If we take away the image, the task must be tackled via linguistic evidence (and common-sense knowledge about scenes) alone. (It hence becomes an inference / entailment task more than one of denotation computation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ],
    "tags": [
     "notebook"
    ]
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import configparser\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import Latex, display\n",
    "\n",
    "pd.set_option('max_colwidth', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "tags": [
     "notebook"
    ]
   },
   "outputs": [],
   "source": [
    "# Load up config file (needs path; adapt env var if necessary); local imports\n",
    "\n",
    "# load config file, set up paths, make project-specific imports\n",
    "config_path = os.environ.get('VISCONF')\n",
    "if not config_path:\n",
    "    # try default location, if not in environment\n",
    "    default_path_to_config = '../../clp-vision/Config/default.cfg'\n",
    "    if os.path.isfile(default_path_to_config):\n",
    "        config_path = default_path_to_config\n",
    "\n",
    "assert config_path is not None, 'You need to specify the path to the config file via environment variable VISCONF.'        \n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config.read_file(f)\n",
    "\n",
    "corpora_base = config.get('DEFAULT', 'corpora_base')\n",
    "preproc_path = config.get('DSGV-PATHS', 'preproc_path')\n",
    "dsgv_home = config.get('DSGV-PATHS', 'dsgv_home')\n",
    "\n",
    "\n",
    "sys.path.append(dsgv_home + '/Utils')\n",
    "from utils import icorpus_code, plot_labelled_bb, get_image_filename, query_by_id\n",
    "from utils import plot_img_cropped, plot_img_ax, invert_dict, get_a_by_b\n",
    "sys.path.append(dsgv_home + '/WACs/WAC_Utils')\n",
    "from wac_utils import create_word2den, is_relational\n",
    "sys.path.append(dsgv_home + '/Preproc')\n",
    "from sim_preproc import load_imsim, n_most_sim\n",
    "\n",
    "sys.path.append('../Common')\n",
    "from data_utils import load_dfs, plot_rel_by_relid, get_obj_bb, compute_distance_objs\n",
    "from data_utils import get_obj_key, compute_relpos_relargs_row, get_all_predicate\n",
    "from data_utils import compute_distance_relargs_row, get_rel_type, get_rel_instances\n",
    "from data_utils import compute_obj_sizes_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "tags": [
     "notebook"
    ]
   },
   "outputs": [],
   "source": [
    "# Load up preprocessed DataFrames. Slow!\n",
    "# These DataFrames are the result of pre-processing the original corpus data,\n",
    "# as per dsg-vision/Preprocessing/preproc.py\n",
    "\n",
    "df_names = ['vgregdf', #'vgimgdf', 'vgobjdf', 'vgreldf',\n",
    "           ]\n",
    "df = load_dfs(preproc_path, df_names)\n",
    "\n",
    "# a derived DF, containing only those region descriptions which I was able to resolve\n",
    "df['vgpregdf'] = df['vgregdf'][df['vgregdf']['pphrase'].notnull() & \n",
    "                               (df['vgregdf']['pphrase'] != '')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co-reference resolution is the task of determining whether a referring expression introduces a new entity into the discourse or not. We can create data for this task using the visual genome region annotation. Turning the set of region descriptions into a \"discourse\", we have gold truth information about whether a region description that is  added to the discourse introduces a new entity or talks about one that has previously been introduced.\n",
    "\n",
    "This is what a model would have to predict. The result then is a set of co-reference chains, or entity mentions (in order of ocurrence). From a more semantic point of view, the task entails determining the size of the intended model of the discourse; co-reference between two mentions here means that only one individual constant needs to be introduced into the model. This is how it is displayed below, with the maximal model size being the number of entity-denoting expressions (if we were to create a new individual constant for each), the minimal number being the number of entity-types in the discourse (and assuming that all mentions of the same type co-refer), and the actual size being the one indicated by the object resolution of the descriptions. A perfect resolution of the co-references would lead to that number. \n",
    "\n",
    "(Note that the example here is relies on the provided object identifiers to distinguish objects, but visual genome seems to have insuffiently consolidated on that score. To create a cleaner dataset, to make the judgement whether a new object is introduced or not, a test of overlap (intersection over union) between bounding boxes should be performed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "The zebra has stripes.\n",
      "    NEW: 2203136 (zebra.n.01)\n",
      "    NEW: 2203137 (band.n.04)\n",
      "    NEW: 2203138 (band.n.04)\n",
      "    old type, new instance: 2203138 band.n.04\n",
      "    NEW: 2203139 (band.n.04)\n",
      "    old type, new instance: 2203139 band.n.04\n",
      "    NEW: 2203140 (band.n.04.)\n",
      "----------\n",
      "The zebra has a black nose.\n",
      "    NEW: 3160396 (zebra.n.01)\n",
      "    old type, new instance: 3160396 zebra.n.01\n",
      "    NEW: 3160397 (nose.n.01.)\n",
      "----------\n",
      "A big tree trunk behind the zebra\n",
      "    NEW: 1746664 (trunk.n.01)\n",
      "    OLD: 2203136 (zebra.n.01)\n",
      "----------\n",
      "a tree behind the zebra\n",
      "    NEW: 3569565 (tree.n.01)\n",
      "    OLD: 2203136 (zebra.n.01)\n",
      "----------\n",
      "a zebra in front of a tree\n",
      "    OLD: 2203136 (zebra.n.01)\n",
      "    OLD: 3569565 (tree.n.01)\n",
      "----------\n",
      "dirt under the zebra\n",
      "    NEW: 3195278 (soil.n.02)\n",
      "    OLD: 2203136 (zebra.n.01)\n",
      "----------\n",
      "the zebra has a striped mane\n",
      "    OLD: 2203136 (zebra.n.01)\n",
      "    NEW: 1729799 (mane.n.01)\n",
      "----------\n",
      "Black and white ears on zebra\n",
      "    NEW: 1943060 (ear.n.01)\n",
      "    NEW: 1943061 (ear.n.01)\n",
      "    old type, new instance: 1943061 ear.n.01\n",
      "    OLD: 2203136 (zebra.n.01)\n",
      "----------\n",
      "Small black eye on the zebra\n",
      "    NEW: 3569557 (eye.n.01)\n",
      "    NEW: 3569558 (eye.n.01)\n",
      "    old type, new instance: 3569558 eye.n.01\n",
      "    OLD: 2203136 (zebra.n.01)\n",
      "----------\n",
      "Black and white stripes on the zebra.\n",
      "    NEW: 3400837 (band.n.04)\n",
      "    old type, new instance: 3400837 band.n.04\n",
      "    OLD: 2203136 (zebra.n.01.)\n",
      "----------\n",
      "Black nose on the zebra.\n",
      "    NEW: 3569559 (nose.n.01)\n",
      "    OLD: 2203136 (zebra.n.01.)\n",
      "----------\n",
      "Roof on the building.\n",
      "    NEW: 3500285 (roof.n.01)\n",
      "    NEW: 3500286 (building.n.01.)\n",
      "----------\n",
      "max model size: 29 || min model size: 13 || actual model size: 19\n",
      "\n",
      "======================================================================\n",
      "beds crisply covered in orange fabric\n",
      "    NEW: 863109 (bed.n.01)\n",
      "    NEW: 863108 (fabric.n.01)\n",
      "----------\n",
      "rectangular white unit below window\n",
      "    NEW: 863116 (unit_of_measurement.n.01)\n",
      "    NEW: 863117 (window.n.01)\n",
      "----------\n",
      "heater up against the wall\n",
      "    OLD: 863116 (heater.n.01)\n",
      "    NEW: 863122 (wall.n.01)\n",
      "----------\n",
      "curtain on the window\n",
      "    NEW: 863112 (curtain.n.01)\n",
      "    OLD: 863117 (window.n.01)\n",
      "----------\n",
      "chair next to the desk\n",
      "    NEW: 863124 (chair.n.01)\n",
      "    NEW: 863125 (desk.n.01)\n",
      "----------\n",
      "laptop on the desk\n",
      "    NEW: 863118 (laptop.n.01)\n",
      "    OLD: 863125 (desk.n.01)\n",
      "----------\n",
      "cup on the desk\n",
      "    NEW: 863126 (cup.n.01)\n",
      "    OLD: 863125 (desk.n.01)\n",
      "----------\n",
      "Single bed in a room\n",
      "    NEW: 863121 (bed.n.01)\n",
      "    old type, new instance: 863121 bed.n.01\n",
      "    NEW: 863129 (room.n.01)\n",
      "----------\n",
      "Wood on the floor\n",
      "    NEW: 863130 (wood.n.01)\n",
      "    NEW: 863115 (floor.n.01)\n",
      "----------\n",
      "Cup on the table next to computer\n",
      "    OLD: 863126 (cup.n.01)\n",
      "    OLD: 863118 (computer.n.01)\n",
      "----------\n",
      "chair next to desk\n",
      "    OLD: 863124 (chair.n.01)\n",
      "    OLD: 863125 (desk.n.01)\n",
      "----------\n",
      "chair next to desk\n",
      "    NEW: 863132 (chair.n.01)\n",
      "    old type, new instance: 863132 chair.n.01\n",
      "    OLD: 863125 (desk.n.01)\n",
      "----------\n",
      "bed in a room\n",
      "    NEW: 863136 (bed.n.01)\n",
      "    old type, new instance: 863136 bed.n.01\n",
      "    OLD: 863129 (room.n.01)\n",
      "----------\n",
      "curtain on a window\n",
      "    OLD: 863112 (curtain.n.01)\n",
      "    OLD: 863117 (window.n.01)\n",
      "----------\n",
      "leg of a chair\n",
      "    NEW: 863137 (leg.n.01)\n",
      "    OLD: 863124 (chair.n.01)\n",
      "----------\n",
      "leg of a chair\n",
      "    NEW: 863138 (leg.n.01)\n",
      "    old type, new instance: 863138 leg.n.01\n",
      "    OLD: 863124 (chair.n.01)\n",
      "----------\n",
      "lamp on a desk\n",
      "    NEW: 863123 (lamp.n.01)\n",
      "    OLD: 863125 (desk.n.01)\n",
      "----------\n",
      "max model size: 34 || min model size: 15 || actual model size: 19\n",
      "\n",
      "======================================================================\n",
      "Bird stand on dry leaves\n",
      "    NEW: 255697 (bird.n.01)\n",
      "    NEW: 255699 (leaf.n.01)\n",
      "----------\n",
      "Bird has intricate feathers\n",
      "    OLD: 255697 (bird.n.01)\n",
      "    NEW: 255700 (feather.n.01)\n",
      "----------\n",
      "Few leaves of grass next to leaves\n",
      "    NEW: 255703 (grass.n.01)\n",
      "    OLD: 255699 (leaf.n.01)\n",
      "----------\n",
      "Black eye of bird\n",
      "    NEW: 255704 (eye.n.01)\n",
      "    OLD: 255697 (bird.n.01)\n",
      "----------\n",
      "curved head of bird\n",
      "    NEW: 255708 (head.n.01)\n",
      "    OLD: 255697 (bird.n.01)\n",
      "----------\n",
      "flat black eye of bird\n",
      "    OLD: 255704 (eye.n.01)\n",
      "    OLD: 255697 (bird.n.01)\n",
      "----------\n",
      "black squiggles on bird's head\n",
      "    OLD: 255697 (bird.n.01's)\n",
      "    OLD: 255708 (head.n.01)\n",
      "----------\n",
      "tiny stem with a few green leaves\n",
      "    NEW: 255710 (root.n.03)\n",
      "    OLD: 255699 (leaf.n.01)\n",
      "----------\n",
      "The bird is standing on the ground.\n",
      "    OLD: 255697 (bird.n.01)\n",
      "    NEW: 255701 (land.n.04.)\n",
      "----------\n",
      "The bird has a black beak.\n",
      "    OLD: 255697 (bird.n.01)\n",
      "    NEW: 255698 (beak.n.01.)\n",
      "----------\n",
      "The bird has black eyes.\n",
      "    OLD: 255697 (bird.n.01)\n",
      "    OLD: 255704 (eye.n.01.)\n",
      "----------\n",
      "The bird has slightly orange legs.\n",
      "    OLD: 255697 (bird.n.01)\n",
      "    NEW: 255705 (leg.n.01.)\n",
      "----------\n",
      "stick behind a bird \n",
      "    NEW: 255709 (stick.n.01)\n",
      "    OLD: 255697 (bird.n.01)\n",
      "----------\n",
      "bird in many dead leaves \n",
      "    OLD: 255697 (bird.n.01)\n",
      "    OLD: 255699 (leaf.n.01)\n",
      "----------\n",
      "stick and leafs \n",
      "    NEW: 255702 (stick.n.01)\n",
      "    old type, new instance: 255702 stick.n.01\n",
      "    OLD: 255699 (leaf.n.01)\n",
      "----------\n",
      "the bird has a beak\n",
      "    OLD: 255697 (bird.n.01)\n",
      "    OLD: 255698 (beak.n.01)\n",
      "----------\n",
      "the bird has an eye\n",
      "    OLD: 255697 (bird.n.01)\n",
      "    OLD: 255704 (eye.n.01)\n",
      "----------\n",
      "the bird has wings\n",
      "    OLD: 255697 (bird.n.01)\n",
      "    NEW: 255707 (wing.n.01)\n",
      "----------\n",
      "the bird has legs\n",
      "    OLD: 255697 (bird.n.01)\n",
      "    OLD: 255705 (leg.n.01)\n",
      "----------\n",
      "max model size: 38 || min model size: 12 || actual model size: 13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# deep caption with co-reference on object level\n",
    "def extr_disc_ref_pphr(pphr):\n",
    "    discourse_referents = []\n",
    "    for token in pphr.split():\n",
    "        subtoken = token.split('|')\n",
    "        if len(subtoken) > 1:\n",
    "            word = subtoken.pop(0)\n",
    "            id_syn_list = zip(subtoken[::2], subtoken[1::2])\n",
    "            discourse_referents.extend([(int(e[0]), e[1]) for e in id_syn_list])\n",
    "    return discourse_referents\n",
    "\n",
    "def cond_print(instr, show):\n",
    "    if show:\n",
    "        print(instr)\n",
    "\n",
    "def model_size_stats(df, image_id, show=False):\n",
    "    all_pphr = df[df['image_id'] == image_id][['phrase', 'pphrase']].values.tolist()\n",
    "    all_discourse_referents = []\n",
    "    all_types = set()\n",
    "    n_mentions = 0\n",
    "    for this_phr, this_pphr in all_pphr:\n",
    "        cond_print(this_phr, show)\n",
    "        this_disc_refs = extr_disc_ref_pphr(this_pphr)\n",
    "        n_mentions += len(this_disc_refs)\n",
    "        #print '   ', this_disc_refs\n",
    "        #this_disc_refs_ids, this_disc_ref_types = zip(*this_disc_refs)\n",
    "        for disc_ref, ref_type in this_disc_refs:\n",
    "            if disc_ref in all_discourse_referents:\n",
    "                cond_print(\"    OLD: %d (%s)\" % (disc_ref, ref_type), show)\n",
    "            else:\n",
    "                cond_print(\"    NEW: %d (%s)\" % (disc_ref, ref_type), show)\n",
    "                all_discourse_referents.append(disc_ref)\n",
    "                if ref_type in all_types:\n",
    "                    cond_print('    old type, new instance: %d %s' % (disc_ref, ref_type), show)\n",
    "                all_types.add(ref_type)\n",
    "        cond_print('-' * 10, show)\n",
    "    cond_print('max model size: %d || min model size: %d || actual model size: %d'\\\n",
    "                    % (n_mentions, len(all_types), len(all_discourse_referents)), show)\n",
    "    return n_mentions, len(all_types), len(all_discourse_referents)\n",
    "\n",
    "n_egs = 3\n",
    "\n",
    "for _ in range(n_egs):\n",
    "    print(\"=\" * 70)\n",
    "    ii = df['vgpregdf'].sample()['image_id'].values[0]\n",
    "    model_size_stats(df['vgpregdf'], ii, show=True)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the examples here show, these aren't particularly nice discourses. Many features of real discourses are missing here: real coherence, in the sense that the individual discourse units build on each other; cohesion, in the sense that discourse-new and discourse-old is properly signalled. But for the purposes here, this can be seen as a feature, as it removes all cues to this task other than semantic ones. To decide whether another mention of an entity type co-refers to a previous one, here a model really must reason about whether the event it occurs in is compatible, what number of entities of this type are likely to be found in a scene of this kind, and so on. This argues that this tasks is still interesting from a semantic perspective, even if a model trained on this data would not directly be transferable to real, natural text. (As a final note, however, it would be possible to annotate the image paragraphs for co-reference and test the model on them, or even train on that data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "author": "Natural Language Semantics with Pictures: Some Language & Vision Datasets and Potential Uses for Computational Semantics",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "../Common/joint.bib",
   "cite_by": "number",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "alt-ctrl-e"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "274px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
