{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*David Schlangen, 2019-03-24*\n",
    "\n",
    "# Task: Predicting Entailments\n",
    "\n",
    "This notebook gives an overview of tasks that make use of images as implicit link between utterances. What follows from the fact that two expressions were provided for the same image (object)? \n",
    "\n",
    "In the perspective established above, where we see images as models of sentences, the question would be \"what follows from being true in the same model\"? In logics, the answer would be \"not much\", as there models are supposed to be stand-ins for the world as a whole (or rather, for one possible world among infinitely many), and many things can be true at the same time, without there being a logical connection between these facts. \n",
    "\n",
    "But our models are perhaps better described as *situations* with some internal coherence stemming from the fact that they are individual slices of the world. (Hence, *situation semantics* \\cite{barwiseperry:sitatt} might have been the more appropriate formalisation to use in the background, but it is a bit more involved than first-order logic, which I used here.) \n",
    "\n",
    "We can thus reformulate our question to: \"what follows from being true in the same situation?\", and investigate whether we can derive a notion of *situational entailment*. This we do in this notebook. The strategy will be to go through the various possible combinations of anchors (image objects) and expression types (referring expressions, captions, etc.) and to inspect the resulting expression pairs. We will also create \"negative\" examples of expressions that might have been taken from the same situation, but weren't. If there is any regularity to the phenomenon, a model of it should be able to distinguish between same-situation pairs and different-situation ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the literature, the relation of interest here is typically called *entailment* or *implication*, which is a more general relation than the *logical* entailment studied in formal semantics. In the most general formulation, a sentence  (lets call it the *hypothesis*) is *implied* by another sentence (or set of sentences; let's call this the *premise*), if accepting the premise makes one (more likely to) accept the hypothesis as well \\cite{chierchi:meaning}. (This is also how later the influential \"recognising textual entailment\" challenge \\cite{Dagan:rte} introduced the relation.)\n",
    "\n",
    "This could be called a pragmatic view on the relation, as it revolves around *accepting* a statement. In formal semantics, one abstracts away from this to yield a universally valid relation (independent of what anyone may or may not choose to accept). Interestingly, there are typically two ways in which the relation can then be explicated. Going the semantic route (and then typically calling the relation *semantic consequence* and using $\\models$ as relation symbol), the notion of truth  as introduced above is harnessed, and the definition becomes \"all models that make the premises true also make the hypothesis true\". Going the syntactic route (and then typically calling the relation *syntactic consequence* and using $\\vdash$ as symbol), the relation is assumed to hold if there is a sequence of applications of syntactic rules that transform the premises into the hypothesis. As the rules are seen as truth-preserving, the idea is that both paths, semantic and syntactic, actually describe the same relation (that is, cover the same pairs of premises and hypotheses). This is the case for some logics, but not all. \n",
    "\n",
    "An interesting task could be to try to set up both \"paths\" (via models / truth and via syntactic transformations) for the tasks described here.\n",
    "\n",
    "Before we launch into the investigation of the data, a few words on related work. The influential \"recognising textual entailments\" challenge was already mentioned above. Under the name \"natural language inference\", the task has recently seen enormous renewed interest. Interestingly, the paper starting this revival, \\cite{snli:emnlp2015}, used image captions as starting point. However, instead of making use of the linked image, they only used the caption as trigger, and asked annotators to *imagine* what must, can, or cannot also be true about the described situation. We use the image to skip the imagination part, having ground truth about whether the situation that makes the hypothesis true is the same or not. But, as we will see, this comes at the cost of what perhaps is noisier data. (We have put some of this data to crowdworkers to let them judge similarity; the experiment has been published in \\cite{schlangen:iwcs19} and is documented in another notebook here.)\n",
    "\n",
    "\\cite{youngetal:flickr30k} pioneered the idea of making use of the image / expression relation, defining a notion of *approximate entailment* and testing it via (sets) of images and partially constructed captions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, one might ask why this is an important notion to model. Here the answer would be that being able to recognise entailment (or weaker forms of it) is crucial for being able to understand discourses, as they are structured by relations between their constituent expressions. While the presentation below will mostly be organised by the types of expressions that are related (where we extend the discussion to relations between expressions of types other than sentence as well), we will also point out where such relations might be found in real discourses, and what the use of being able to recognise them would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ],
    "tags": [
     "notebook"
    ]
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import configparser\n",
    "import os\n",
    "import random\n",
    "from textwrap import fill\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import Latex, display\n",
    "\n",
    "pd.set_option('max_colwidth', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "tags": [
     "notebook"
    ]
   },
   "outputs": [],
   "source": [
    "# Load up config file (needs path; adapt env var if necessary); local imports\n",
    "\n",
    "# load config file, set up paths, make project-specific imports\n",
    "config_path = os.environ.get('VISCONF')\n",
    "if not config_path:\n",
    "    # try default location, if not in environment\n",
    "    default_path_to_config = '../../clp-vision/Config/default.cfg'\n",
    "    if os.path.isfile(default_path_to_config):\n",
    "        config_path = default_path_to_config\n",
    "\n",
    "assert config_path is not None, 'You need to specify the path to the config file via environment variable VISCONF.'        \n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config.read_file(f)\n",
    "\n",
    "corpora_base = config.get('DEFAULT', 'corpora_base')\n",
    "preproc_path = config.get('DSGV-PATHS', 'preproc_path')\n",
    "dsgv_home = config.get('DSGV-PATHS', 'dsgv_home')\n",
    "\n",
    "\n",
    "sys.path.append(dsgv_home + '/Utils')\n",
    "from utils import icorpus_code, plot_labelled_bb, get_image_filename, query_by_id\n",
    "from utils import plot_img_cropped, plot_img_ax, invert_dict, get_a_by_b\n",
    "sys.path.append(dsgv_home + '/WACs/WAC_Utils')\n",
    "from wac_utils import create_word2den, is_relational\n",
    "sys.path.append(dsgv_home + '/Preproc')\n",
    "from sim_preproc import load_imsim, n_most_sim\n",
    "\n",
    "sys.path.append('../Common')\n",
    "from data_utils import load_dfs, plot_rel_by_relid, get_obj_bb, compute_distance_objs\n",
    "from data_utils import get_obj_key, compute_relpos_relargs_row, get_all_predicate\n",
    "from data_utils import compute_distance_relargs_row, get_rel_type, get_rel_instances\n",
    "from data_utils import compute_obj_sizes_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "tags": [
     "notebook"
    ]
   },
   "outputs": [],
   "source": [
    "# Load up preprocessed DataFrames. Slow!\n",
    "# These DataFrames are the result of pre-processing the original corpus data,\n",
    "# as per dsg-vision/Preprocessing/preproc.py\n",
    "\n",
    "df_names = [#'saiapr_bbdf', 'saiapr_refdf',\n",
    "            'mscoco_bbdf', 'refcoco_refdf', 'refcocoplus_refdf', 'grex_refdf',\n",
    "            'vgregdf', 'vgimgdf', 'vgobjdf', 'vgreldf',\n",
    "            'vgpardf', 'cococapdf']\n",
    "            # 'flickr_bbdf', 'flickr_capdf', 'flickr_objdf']\n",
    "df = load_dfs(preproc_path, df_names)\n",
    "\n",
    "# a derived DF, containing only those region descriptions which I was able to resolve\n",
    "df['vgpregdf'] = df['vgregdf'][df['vgregdf']['pphrase'].notnull() & \n",
    "                               (df['vgregdf']['pphrase'] != '')]\n",
    "\n",
    "# load up pre-computed similarities\n",
    "coco_sem_sim, coco_sem_map = load_imsim(os.path.join(preproc_path, 'mscoco_sim.npz'))\n",
    "visg_sem_sim, visg_sem_map = load_imsim(os.path.join(preproc_path, 'visgen_sim.npz'))\n",
    "coco_id2semsim = invert_dict(coco_sem_map)\n",
    "visg_id2semsim = invert_dict(visg_sem_map)\n",
    "\n",
    "coco_vis_sim, coco_vis_map = load_imsim(os.path.join(preproc_path, 'mscoco_vis_sim.npz'))\n",
    "visg_vis_sim, visg_vis_map = load_imsim(os.path.join(preproc_path, 'visgen_vis_sim.npz'))\n",
    "coco_id2vissim = invert_dict(coco_vis_map)\n",
    "visg_id2vissim = invert_dict(visg_vis_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we fully delve into this, however, we first look how the data might be used to learn representations for word meanings that might support the task of inferring entailment relations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Word Representations from Referential Uses\n",
    "\n",
    "There is a long tradition of work on word meaning where it is modelled not via denotations (as in the *denotations* part of this notebook), but rather via contexts of use, as recorded in corpora (see survey in \\cite{turney-pantel:10}), and where the central notion for which it is put to work is not *truth* relative to a model, but rather the (somewhat vaguer) notion of semantic *similarity*. This tradition has recently been refreshed by the advent of more powerful methods for learning these representations from corpora \\cite{Mikolov2013:embeddings}.\n",
    "\n",
    "The context of use that are the basis of these approaches typically are only linguistic contexts as found in text corpora. The image corpora discussed here open up the possiblitiy to structure the context further, by the referential uses that were made of the expressions. For example, we can look at which words occur together in references to an object (*within context* use), and distinguish them from words that don't (*outside* words). (We report results for such an approach in \\cite{zaschla:contground}, showing that for visual referntial similarity, it outperforms purely textually trained representations.)\n",
    "\n",
    "We show some examples here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target word: painted\n",
      "belonging to same context:\n",
      "    right, meter, parked, blue, lights, pay, machine, to, tail, station,\n",
      "plate, accord, showing, honda, dark, with, rear, a, on, license, car,\n",
      "near, of, the, side, road\n",
      "belonging to different context:\n",
      "    pink, right, center, of, between, bowl, middle, giraffe, woman, pic,\n",
      "in, soldiers, chair, white, empty, kid\n",
      "----------------------------------------\n",
      "target word: attached\n",
      "belonging to same context:\n",
      "    smaller, is, one, down, second, in, sandwiched, from, top, two, black,\n",
      "largest, wheels, suitcases, sided, under, with, between, a, on,\n",
      "stacked, luggage, grey, cat, large, bag, suitcase, the, soft\n",
      "belonging to different context:\n",
      "    wheel, on, picture, right, boy, of, bottom, young, side, suv, parts,\n",
      "black, motorcycle, girl, other, kid\n"
     ]
    }
   ],
   "source": [
    "# referential context\n",
    "def get_all_refexps(corps, id_triple):\n",
    "    this_refexp = []\n",
    "    for this_corp in coco_ref_corps:\n",
    "        this_refexp.extend(query_by_id(df[this_corp], id_triple, column='refexp'))\n",
    "    return this_refexp\n",
    "\n",
    "coco_ref_corps = ['refcoco_refdf', 'refcocoplus_refdf', 'grex_refdf']\n",
    "\n",
    "min_length_target = 5\n",
    "\n",
    "# Example 1\n",
    "targt_triple = df['refcoco_refdf'].sample()['i_corpus image_id region_id'.split()].values[0]\n",
    "targt_refexp = get_all_refexps(coco_ref_corps, targt_triple)\n",
    "\n",
    "distr_refexps = df['refcoco_refdf'].sample(5)['refexp'].tolist()\n",
    "\n",
    "listA = list(set(' '.join(targt_refexp).split()))\n",
    "listB = list(set(' '.join(distr_refexps).split()))\n",
    "\n",
    "target_word = \"\"\n",
    "while len(target_word) < min_length_target:\n",
    "    target_word_index = random.choice(range(len(listA)))\n",
    "    target_word = listA[target_word_index]\n",
    "_ = listA.pop(target_word_index)\n",
    "\n",
    "print(\"target word:\", target_word)\n",
    "print(\"belonging to same context:\")\n",
    "print('   ', fill(', '.join(listA), 70))\n",
    "print(\"belonging to different context:\")\n",
    "print('   ', fill(', '.join(listB), 70))\n",
    "\n",
    "print('-' * 40)\n",
    "# Example 2\n",
    "targt_triple = df['refcoco_refdf'].sample()['i_corpus image_id region_id'.split()].values[0]\n",
    "targt_refexp = get_all_refexps(coco_ref_corps, targt_triple)\n",
    "\n",
    "distr_refexps = df['refcoco_refdf'].sample(5)['refexp'].tolist()\n",
    "\n",
    "listA = list(set(' '.join(targt_refexp).split()))\n",
    "listB = list(set(' '.join(distr_refexps).split()))\n",
    "\n",
    "target_word = \"\"\n",
    "while len(target_word) < min_length_target:\n",
    "    target_word_index = random.choice(range(len(listA)))\n",
    "    target_word = listA[target_word_index]\n",
    "_ = listA.pop(target_word_index)\n",
    "\n",
    "print(\"target word:\", target_word)\n",
    "print(\"belonging to same context:\")\n",
    "print('   ', fill(', '.join(listA), 70))\n",
    "print(\"belonging to different context:\")\n",
    "print('   ', fill(', '.join(listB), 70))\n",
    "\n",
    "#pos = [tuple(np.random.choice(listA, 2)) for _ in range(10)]\n",
    "#neg = [(np.random.choice(listA), np.random.choice(listB)) for _ in range(10)]\n",
    "#\n",
    "#print \"From same context:\"\n",
    "#for pair in pos:\n",
    "#    print '  {:>10} , {:<10}'.format(pair[0], pair[1])\n",
    "#print \"\"\n",
    "#print \"From different contexts:\"\n",
    "#for pair in neg:\n",
    "#    print '  {:>10} , {:<10}'.format(pair[0], pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this indicates, this method is probably more likely to result in useful representations for nouns and adjectives than for other parts of speech (as can be expected, since the referential function is crucial here).\n",
    "\n",
    "Also, readers that are familiar with this approach will have already seen that this is likely to push terms apart that in other aspects would be seen as semantically very close (e.g., different colours), if they are incompatible on the level of instances of reference (since for example in the corpora it will be rare for the same object to be called both \"black\" and \"red\", or \"large\" and \"small\", or \"man\" and \"woman\"). (For more details, see \\cite{zaschla:contground}.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again summarise properties of the dataset that could be derived from the available data in this way:\n",
    "\n",
    "* **Dataset:** words paired with contexts (words from same referring expression)\n",
    "* **Negative Instances:** words from different referring expression\n",
    "* **Source:** referring expression corpora\n",
    "* **Uses:** learning word representations optimized for *referential* similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicature / Approximate Entailment\n",
    "\n",
    "We extend this approach of using the non-linguistic context to create pairings now to larger expressions, and move from semantic *similarity* to *implication* (or *entailment*, or, to introduce yet another term, to *approximate entailment*, which is how \\cite{youngetal:flickr30k} introduced this task [for captions]; where the qualifier \"approximate\" is added presumably to express the fact that this isn't quite *logical* entailment).\n",
    "\n",
    "The general approach here will be to take positive examples from the set of annotations for the same object (that is, expressions that are related to the same image); for example, two expressions referring to the same image object, or two captions describing the same image. As we will see, this will indeed only yield pairs that are likely to be evaluated similarly in *all* context (more likely in any case than the negative pairs); but generalising this from the fact that this is the case for *one* context is of course potentially fallible. Similarly, an expression taken from another image is only likely not to accidentally apply to the same situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... between Referring Expressions\n",
    "\n",
    "The following shows some example pairings of referring expressions taken from the same object (premise + p-hyp) or from a different object (premise + n-hyp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "premise:  man beer\n",
      "p-hyp:    person in entire picture\n",
      "n-hyp:    chair\n",
      "----------------------------------------\n",
      "premise:  kid red tie\n",
      "p-hyp:    red tie\n",
      "n-hyp:    left donkey\n",
      "----------------------------------------\n",
      "premise:  black dog right\n",
      "p-hyp:    black dog purple collar front\n",
      "n-hyp:    guy on left side\n",
      "----------------------------------------\n",
      "premise:  cat left\n",
      "p-hyp:    left cat\n",
      "n-hyp:    red veh top right\n",
      "----------------------------------------\n",
      "premise:  white horse\n",
      "p-hyp:    white horse\n",
      "n-hyp:    the boy second from right red tie\n",
      "----------------------------------------\n",
      "premise:  skiier\n",
      "p-hyp:    skiier\n",
      "n-hyp:    green stripe board\n",
      "----------------------------------------\n",
      "premise:  left picture\n",
      "p-hyp:    left tie\n",
      "n-hyp:    left skier\n",
      "----------------------------------------\n",
      "premise:  fridge behind man\n",
      "p-hyp:    the refrigerator on the right\n",
      "n-hyp:    partially visible head to the right of red hat in middle\n",
      "----------------------------------------\n",
      "premise:  girl in green shirt yellow pants\n",
      "p-hyp:    woman in green shirt yellow pants\n",
      "n-hyp:    blue and white semi\n",
      "----------------------------------------\n",
      "premise:  right zebra\n",
      "p-hyp:    right\n",
      "n-hyp:    left horse\n"
     ]
    }
   ],
   "source": [
    "# pairing premise and hypotheses\n",
    "n = 10 # how many to do\n",
    "triples = []\n",
    "\n",
    "this_df = df['refcoco_refdf']\n",
    "\n",
    "for _ in range(n):\n",
    "    # seed image\n",
    "    ic, ii, ri, rexi = this_df.sample()['i_corpus image_id region_id rex_id'.split()].values[0]\n",
    "    premise, phyp =  np.random.choice(query_by_id(this_df, (ic, ii, ri), 'refexp'), 2, replace=False)\n",
    "\n",
    "    # negative hypothesis\n",
    "    nhyp = this_df.sample()['refexp'].values[0]\n",
    "    triples.append((premise, phyp, nhyp))\n",
    "\n",
    "colnames = 'premise p-hyp n-hyp'.split()\n",
    "#pd.DataFrame(triples, columns=colnames)\n",
    "\n",
    "for prem, phyp, nhyp in triples:\n",
    "    print(\"-\" * 40)\n",
    "    print(\"premise: \", prem)\n",
    "    print(\"p-hyp:   \", phyp)\n",
    "    print(\"n-hyp:   \", nhyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, distinguishing between these pairs seems to be a rather easy task, for which attention to lexical items might even be enough. *Explaining* the decision, however, would not be trivial and require knowledge about how speakers are likely to refer, or about what properties are unlikely to co-occur in the same entity. \n",
    "\n",
    "Relating this abstract task to a real(er) discourse task, we can phrase this as recognising whether something can serve as an answer to a clarification request. The positive hypotheses shown here would work as elaborations or reformulations of the description that the premise gives; it is harder to understand the negative hypotheses in that way. \n",
    "\n",
    "Here are some examples put into this kind of context:\n",
    "\n",
    "+ A: black car behind dorks holding signs  \n",
    "  B: what?  \n",
    "  A: the car behind the three people\n",
    "\n",
    "vs \n",
    "\n",
    "+ A: black car behind dorks holdings signs  \n",
    "  B: what?  \n",
    "  A: #closer girl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... between Captions and \"There is\"-Sentences\n",
    "\n",
    "Where the pairs above combined expressions whose denotations are on the same level, as it were (objects and objects), we can also create unequal pairings. Using a caption as hypothesis (a description of the situation as a whole), we can ask whether it entails the presence of specific objects.\n",
    "\n",
    "Here are some examples of caption paired with an object referred to via a name (slotted into the \"there is __\" frame):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ],
    "tags": [
     "notebook"
    ]
   },
   "outputs": [],
   "source": [
    "# intersecting visual genome and coco captions. Slow-ish.\n",
    "caption_coco_iids = list(set(df['cococapdf']['image_id'].tolist()))\n",
    "# regions for only those image for which we also have coco captions\n",
    "visgencocap_regdf = df['vgregdf'].merge(pd.DataFrame(caption_coco_iids, columns=['coco_id']))\n",
    "# coco_image_ids for images with both caption and region\n",
    "vgcap_coco_iids = list(set(visgencocap_regdf['coco_id'].tolist()))\n",
    "# visgen_image_ids for images with both caption and region\n",
    "vgcap_vg_iids = list(set(visgencocap_regdf['image_id'].tolist()))\n",
    "\n",
    "# map coco_ids to visgen_ids, and back\n",
    "coco2vg = dict(visgencocap_regdf[['coco_id', 'image_id']].values)\n",
    "vg2coco = dict([(v,k) for k,v in coco2vg.items()])\n",
    "\n",
    "df['vgpardf']['coco_image_id'] = df['vgpardf']['image_id'].apply(lambda x: vg2coco.get(x, None))\n",
    "df['cocoparcapdf'] = df['cococapdf'].merge(df['vgpardf'],\n",
    "                                           left_on='image_id', right_on='coco_image_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "premise:  A group of people are out taking a baby for a walk.\n",
      "p-hyp:    there is a head\n",
      "n-hyp:    there is a fence\n",
      "----------------------------------------\n",
      "premise:  A young person with outstretched arms in a driveway\n",
      "p-hyp:    there is a shirt\n",
      "n-hyp:    there is a foot\n",
      "----------------------------------------\n",
      "premise:  a green fire hydrant standing in the grass close to the street\n",
      "p-hyp:    there is a buillding\n",
      "n-hyp:    there is a match\n",
      "----------------------------------------\n",
      "premise:  A clay rendition of roses in a pot are displayed.\n",
      "p-hyp:    there is a tube\n",
      "n-hyp:    there is a concrete planters\n",
      "----------------------------------------\n",
      "premise:  three people are waiting on the street with an umbrella\n",
      "p-hyp:    there is a rollers\n",
      "n-hyp:    there is a letters\n",
      "----------------------------------------\n",
      "premise:  A large jetliner flying through a blue sky.\n",
      "p-hyp:    there is a wheels\n",
      "n-hyp:    there is a clock\n",
      "----------------------------------------\n",
      "premise:  A man in traditional Mexican grad stands under a rainbow umbrella\n",
      "p-hyp:    there is a man\n",
      "n-hyp:    there is a vehicle\n",
      "----------------------------------------\n",
      "premise:  Sun loungers, straw umbrellas and the blue ocean await the tourists\n",
      "p-hyp:    there is a this\n",
      "n-hyp:    there is a human\n",
      "----------------------------------------\n",
      "premise:  A boy is waiting by a train and train tracks\n",
      "p-hyp:    there is a beautiful photo.\n",
      "n-hyp:    there is a letter\n",
      "----------------------------------------\n",
      "premise:  a large plane a small plane a bus and work trucks on the airfield\n",
      "p-hyp:    there is a tail fin\n",
      "n-hyp:    there is a vest\n"
     ]
    }
   ],
   "source": [
    "# captions and objects (slotted into \"there is __\" frame)\n",
    "tuples = []\n",
    "\n",
    "for _ in range(10):\n",
    "    try:\n",
    "        vgii, cocoii = visgencocap_regdf.sample()['image_id coco_id'.split()].values[0]\n",
    "        prem = df['cococapdf'][df['cococapdf']['image_id'] == cocoii].sample()['caption'].values[0]\n",
    "        phyp = df['vgobjdf'][df['vgobjdf']['image_id'] == vgii].sample()['name'].values[0]\n",
    "        nhyp = df['vgobjdf'].sample()['name'].values[0]\n",
    "        tuples.append((prem, phyp, nhyp))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "for prem, phyp, nhyp in tuples:\n",
    "    print(\"-\" * 40)\n",
    "    print(\"premise: \", fill(prem, 70))\n",
    "    print(\"p-hyp:    there is a\", phyp)\n",
    "    print(\"n-hyp:    there is a\", nhyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the same configuration, but with whole region descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "premise:  a number of people siting at a table with wine glasses\n",
      "p-hyp:    there is/are (a) young woman with long wavy brown hair\n",
      "n-hyp:    there is/are (a) the hand of a boy\n",
      "========================================\n",
      "premise:  A woman and a black cat outside a building.\n",
      "p-hyp:    there is/are (a) wire running along the wall\n",
      "n-hyp:    there is/are (a) table the cake is on\n",
      "========================================\n",
      "premise:  Group of people watching someone play a video game.\n",
      "p-hyp:    there is/are (a) red flowers behind a couch\n",
      "n-hyp:    there is/are (a) house with a light on\n",
      "========================================\n",
      "premise:  a man with his dog catching a red frisbee\n",
      "p-hyp:    there is/are (a) a man wearing glasses\n",
      "n-hyp:    there is/are (a) man wearing white headband\n",
      "========================================\n",
      "premise:  Large bird flying and gliding over the ocean.\n",
      "p-hyp:    there is/are (a) specks on bird \n",
      "n-hyp:    there is/are (a) The left handle brake on the motorcycle.\n",
      "========================================\n",
      "premise:  A skateboarder touching the ground with his hands\n",
      "p-hyp:    there is/are (a) The man has khaki pants\n",
      "n-hyp:    there is/are (a) white horse with purple outfit looking at the camera\n",
      "========================================\n",
      "premise:  A group of men wearing dress clothes and exposing their socks.\n",
      "p-hyp:    there is/are (a) man on the deck\n",
      "n-hyp:    there is/are (a) Clock is on shelf\n",
      "========================================\n",
      "premise:  several cakes and cupcakes on a table at a party\n",
      "p-hyp:    there is/are (a) two red, white, and blue plates with cakes on them\n",
      "n-hyp:    there is/are (a) yellow vest on the man\n",
      "========================================\n",
      "premise:  A clock tower stands on a cloudy day.\n",
      "p-hyp:    there is/are (a) Clocks on a tower\n",
      "n-hyp:    there is/are (a) two donuts with holes\n"
     ]
    }
   ],
   "source": [
    "# caption + region description\n",
    "for _ in range(10):\n",
    "    try:\n",
    "        p_caps = []\n",
    "        while len(p_caps) == 0:\n",
    "            coco_ii = np.nan\n",
    "            while np.isnan(coco_ii):\n",
    "                ic, vg_ii, coco_ii = df['vgimgdf'].sample()[['i_corpus', 'image_id', 'coco_id']].values[0]\n",
    "            p_caps = query_by_id(df['cococapdf'], (icorpus_code['mscoco'], coco_ii))\n",
    "        p_cap_ind = random.choice(range(len(p_caps)))\n",
    "        p_cap = p_caps.iloc[p_cap_ind]['caption']\n",
    "        p_row = p_caps.index[p_cap_ind]\n",
    "\n",
    "        p_hyp_regions = query_by_id(df['vgpregdf'], (ic, vg_ii))\n",
    "        p_hyp_regions = p_hyp_regions[~p_hyp_regions['rels'].isnull()]\n",
    "        p_hyp_reg, p_hyp_relids, p_hyp_rels, p_hyp_pphrase = \\\n",
    "            p_hyp_regions.sample()['phrase rel_ids rels pphrase'.split()].values[0]\n",
    "\n",
    "        n_hyp_reg, n_hyp_relids, n_hyp_rels, n_hyp_pphrase = \\\n",
    "            df['vgpregdf'].sample()['phrase rel_ids rels pphrase'.split()].values[0]\n",
    "\n",
    "        print(\"=\" * 40)\n",
    "        print(\"premise: \", fill(p_cap, 70))\n",
    "        print(\"p-hyp:    there is/are (a)\", p_hyp_reg) #, '||', p_hyp_pphrase\n",
    "        print(\"n-hyp:    there is/are (a)\", n_hyp_reg) #, '||', n_hyp_pphrase\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the examples indicate, this configuration seems to create examples that bring out a rather clear version of (approximate, commonsense) entailment (perhaps better called *situational implication*): Does a situation of the type described by the caption typically entail / imply the presence of an object of the type described by the hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... between Captions\n",
    "\n",
    "Stepping up the complexity of the expressions, here are pairings based on captions and full images. (Note that this, as mentioned above, was also the starting point of the now popular \"natural language inference\" datasets \\cite{snli:emnlp2015}, where the positive pair was taken from COCO captions. The negative pair and the additional \"neutral\" pair, however, were created manually rather than how it is done here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "premise:  A large and a small giraffe standing in the grass\n",
      "p-hyp:    An adult giraffe and a baby giraffe standing next to each other in the\n",
      "woods.\n",
      "n-hyp:    A boat on a body of water with trees in the background.\n",
      "----------------------------------------\n",
      "premise:  Two tennis players are shaking hands over the net.\n",
      "p-hyp:    Two men are shaking hands at a tennis game\n",
      "n-hyp:    A young boy with an adult umbrella stands near a stroller.\n",
      "----------------------------------------\n",
      "premise:  A dog looking out the window of a car.\n",
      "p-hyp:    A dog with its face handing out a car window.\n",
      "n-hyp:    some kind of white and brown desert on a table\n",
      "----------------------------------------\n",
      "premise:  This shows a young man's jump as continuous sequence of actions.\n",
      "p-hyp:    A picture of a snowboarder jumping right into the air.\n",
      "n-hyp:    A woman standing on a street with a umbrella.\n",
      "----------------------------------------\n",
      "premise:  A woman takes a bite of the whole pizza.\n",
      "p-hyp:    Child holding up a pizza pie to her face in a restaurant.\n",
      "n-hyp:    THREE MEN IN A GARAGE LOOKING OVER A MOTORCYCLE\n",
      "----------------------------------------\n",
      "premise:  Some elephants in the middle of a tree covered area\n",
      "p-hyp:    An elephant is photographed standing in the dirt.\n",
      "n-hyp:    A man is trying his best to catch the upcoming ball with his MIT.\n",
      "----------------------------------------\n",
      "premise:  A white toilet sitting in the corner of a bathroom.\n",
      "p-hyp:    A bathroom area with a toilet, safety rails and wall art.\n",
      "n-hyp:    A herd of horses grazing in a field.\n",
      "----------------------------------------\n",
      "premise:  Black and white photo of two refrigerators below an escalator.\n",
      "p-hyp:    Two old refrigerators that have been sealed out on the sidewalk for\n",
      "pickup.\n",
      "n-hyp:    A pizza sitting on top of a stove top.\n",
      "----------------------------------------\n",
      "premise:  A reflection  fo a man walking across a street while holding an\n",
      "umbrella.\n",
      "p-hyp:    The reflection of a man with an umbrella on wet surface\n",
      "n-hyp:    A cupcake with many marsh mellows on top of it.\n",
      "----------------------------------------\n",
      "premise:  A cat peeking into a room with a tiled floor from the doorway.\n",
      "p-hyp:    there is a cat that is peaking through the curtain\n",
      "n-hyp:    He is long and lean like his skateboard.\n"
     ]
    }
   ],
   "source": [
    "# pairing premise and hypotheses\n",
    "n = 10 # how many to do\n",
    "triples = []\n",
    "\n",
    "this_df = df['cococapdf']\n",
    "\n",
    "for _ in range(n):\n",
    "    # seed image\n",
    "    ic, ii, rexi = this_df.sample()['i_corpus image_id id'.split()].values[0]\n",
    "    premise, phyp =  np.random.choice(query_by_id(this_df, (ic, ii), 'caption'), 2, replace=False)\n",
    "\n",
    "    # negative hypothesis\n",
    "    nhyp = this_df.sample()['caption'].values[0]\n",
    "    triples.append((premise, phyp, nhyp))\n",
    "\n",
    "#colnames = 'premise p-hyp n-hyp'.split()\n",
    "#pd.DataFrame(triples, columns=colnames)\n",
    "\n",
    "for prem, phyp, nhyp in triples:\n",
    "    print(\"-\" * 40)\n",
    "    print(\"premise: \", fill(prem, 70))\n",
    "    print(\"p-hyp:   \", fill(phyp, 70))\n",
    "    print(\"n-hyp:   \", fill(nhyp, 70))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these examples, the negative hypotheses were selected simply by sampling captions of images other than that from which the premise was taken. Again, to a human reader, the negative hypotheses seem to jump out, and it is not unlikely that a rather shallow model (looking only at semantic similarity of the words and ignoring compositionality) could perform well on this task. But again, *explaining* why the descriptions could be of the same situation (or not), seems like a challenging task, requiring knowledge about event types as well as knowledge about entity types.\n",
    "\n",
    "In any case, we can make the task more challenging, by using our similarity relation between images to select the distractor image from which the negative hypothesis caption is to be taken. The assumption here would be that descriptions of more similar situations should also be more similar, and that to distinguish between them, a deeper understanding of the expression itself is needed.\n",
    "\n",
    "Here are examples using the semantic similarity relation described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "premise:  a couple of people on some skies on a snowy field\n",
      "p-hyp:    an Olympic snow skier a judge and a spectator \n",
      "n-hyp:    Two people in orange jackets smile as they ski up a road.\n",
      "----------------------------------------\n",
      "premise:  A couple of zebras are standing in a field\n",
      "p-hyp:    an image of two zebras in the wild\n",
      "n-hyp:    A group of zebras are near the side of the road, with one on its back. \n",
      "----------------------------------------\n",
      "premise:  Two young men playing frisbee in a park.\n",
      "p-hyp:    this is two men in cleats on the grass\n",
      "n-hyp:    Several people in black red and green are on a field as one person wears white shorts and two people have their arms up toward a white Frisbee.\n",
      "----------------------------------------\n",
      "premise:  The skateboarder jumps over the ramp on his board.\n",
      "p-hyp:    A boy doing a trick on a skateboard from a ramp.\n",
      "n-hyp:    There are men who are skateboarding down the trail.\n",
      "----------------------------------------\n",
      "premise:  a baseball player is leaning back with a ball\n",
      "p-hyp:    The baseball player wearing a blue cap and jersey is throwing a pitch.\n",
      "n-hyp:    A baseball player tries to avoid a tag out play.\n",
      "----------------------------------------\n",
      "premise:  a beach filled with some colorful umbrellas and boats \n",
      "p-hyp:    a number of small boats on a beach near an umbrella\n",
      "n-hyp:    A fleet of boats sailing on a body of water.\n",
      "----------------------------------------\n",
      "premise:  A man milking a cow with another cow standing close by \n",
      "p-hyp:    a person milking a cow next to a wall\n",
      "n-hyp:    A cow looking into the camera and wearing a chain.\n",
      "----------------------------------------\n",
      "premise:  A group of zebras walking in a grassy savanna.\n",
      "p-hyp:    some zebras standing together, with one looking in a different direction than the others \n",
      "n-hyp:    Three zebras by a wooded area eating grass. \n",
      "----------------------------------------\n",
      "premise:  Brown cows are standing in a field of grass.\n",
      "p-hyp:    Tagged cows grazing in an open mountain pasture\n",
      "n-hyp:    A line of black and white cows are lined up and grazing.\n",
      "----------------------------------------\n",
      "premise:  A manual in Spanish next to a Nokia phone.\n",
      "p-hyp:    A book laying open on a desk with a cell phone.\n",
      "n-hyp:    A little girl is holding a Minnie Mouse umbrella above her head. \n"
     ]
    }
   ],
   "source": [
    "# pairing premise and hypotheses\n",
    "n = 10 # how many to do\n",
    "tuples = []\n",
    "\n",
    "this_df = df['cococapdf']\n",
    "ic = icorpus_code['mscoco']\n",
    "\n",
    "for _ in range(n):\n",
    "    # seed image\n",
    "    ii = np.random.choice(coco_id2semsim.keys())\n",
    "    premise, phyp =  np.random.choice(query_by_id(this_df, (ic, ii), 'caption'), 2, replace=False)\n",
    "\n",
    "    # negative hypothesis\n",
    "    sim_ids = n_most_sim(coco_sem_sim, coco_sem_map, coco_id2semsim[ii], n=5)\n",
    "    n_ii = np.random.choice(sim_ids[1:])\n",
    "    nhyp = this_df[this_df['image_id'] == n_ii]['caption'].values[0]\n",
    "    \n",
    "    tuples.append((premise, phyp, nhyp))\n",
    "\n",
    "# colnames = 'premise p-hyp n-hyp'.split()\n",
    "# pd.DataFrame(tuples, columns=colnames)\n",
    "\n",
    "for prem, phyp, nhyp in tuples:\n",
    "    print(\"-\" * 40)\n",
    "    print(\"premise: \", prem)\n",
    "    print(\"p-hyp:   \", phyp)\n",
    "    print(\"n-hyp:   \", nhyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same with the visual similarity relation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "premise:  A man sitting at a table as he stares at his mobile phone.\n",
      "p-hyp:    He is eating lunch with his mobile phone on the tray.\n",
      "n-hyp:    A woman that is standing near a counter.\n",
      "----------------------------------------\n",
      "premise:  THERE ARE SEVERAL MOTOR BIKES PARKED ON THE STREET \n",
      "p-hyp:    The back ends of a line of parked motorcycles.\n",
      "n-hyp:    Some motorcycles and scooters parked in front of some tents. \n",
      "----------------------------------------\n",
      "premise:  Two toddlers are playing with an electric organ.\n",
      "p-hyp:    A person sitting at a computer with two kids and one is pushing keys.\n",
      "n-hyp:    This little girl can't wait to get a piece of this cake\n",
      "----------------------------------------\n",
      "premise:  Costumed men riding in a horse drawn carriage at a fair\n",
      "p-hyp:    The carriage was being pulled by two horses. \n",
      "n-hyp:    A man sitting on a wagon seat driving two draft horses\n",
      "----------------------------------------\n",
      "premise:  Oatmeal sitting on top of a black table in white bowls. \n",
      "p-hyp:    three bowls filled with food and some silverware\n",
      "n-hyp:    A bowl of food including two hard boiled eggs cut in half.\n",
      "----------------------------------------\n",
      "premise:  a living room with a couch and a fire place\n",
      "p-hyp:    an empty living room is well decorated with furniture and a clock\n",
      "n-hyp:    A den with a chair, couch, coffee table and mirror.\n",
      "----------------------------------------\n",
      "premise:  Five men posing together with a white frisbee. \n",
      "p-hyp:    A group of guys posing with a disc.\n",
      "n-hyp:    Two little girls holding teddy bears with bear ears and a bear nose on.\n",
      "----------------------------------------\n",
      "premise:  Small dog looking past glass of wine with adults nearby.\n",
      "p-hyp:    A dog with a wine glass being held to its face. \n",
      "n-hyp:    A woman sitting at a wooden table next to a white cat.\n",
      "----------------------------------------\n",
      "premise:  A herd of sheep in a stall of a field.\n",
      "p-hyp:    a couple of sheep that are in side a fence\n",
      "n-hyp:    A group of sheep standing next to a wall in a building.\n",
      "----------------------------------------\n",
      "premise:  A family sitting on the shore of a lake at sunset.\n",
      "p-hyp:    Several people sitting in the sand on the beach together.\n",
      "n-hyp:    Several men are talking with a soldier about the nearby truck in the ditch.\n"
     ]
    }
   ],
   "source": [
    "# pairing premise and hypotheses\n",
    "n = 10 # how many to do\n",
    "tuples = []\n",
    "\n",
    "this_df = df['cococapdf']\n",
    "ic = icorpus_code['mscoco']\n",
    "\n",
    "for _ in range(n):\n",
    "    # seed image\n",
    "    ii = np.random.choice(coco_id2semsim.keys())\n",
    "    premise, phyp =  np.random.choice(query_by_id(this_df, (ic, ii), 'caption'), 2, replace=False)\n",
    "\n",
    "    # negative hypothesis\n",
    "    sim_ids = n_most_sim(coco_vis_sim, coco_vis_map, coco_id2vissim[ii], n=5)\n",
    "    n_ii = np.random.choice(sim_ids[1:])\n",
    "    nhyp = this_df[this_df['image_id'] == n_ii]['caption'].values[0]\n",
    "    \n",
    "    tuples.append((premise, phyp, nhyp))\n",
    "\n",
    "#colnames = 'premise p-hyp n-hyp'.split()\n",
    "#pd.DataFrame(tuples, columns=colnames)\n",
    "\n",
    "for prem, phyp, nhyp in tuples:\n",
    "    print(\"-\" * 40)\n",
    "    print(\"premise: \", prem)\n",
    "    print(\"p-hyp:   \", phyp)\n",
    "    print(\"n-hyp:   \", nhyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As these examples illustrate, the similarity might even sometimes be too great, so that the assumption that the negative hypothesis is less likely to also work is broken. It looks like some fine tuning (and testing on human raters) would be needed to find the right degree of (dis)similarity to produce a challenging but promising dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... between Captions and Paragraphs\n",
    "\n",
    "The expressions do not need to be of the same type; the more important requirement is that they relate to the same type of object. Here we show captions as premise, and image paragraphs as hypotheses (with the negative instances coming from similar images). The hypotheses thus could be seen as elaborations of the situation described by the caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "A hotel wall with a white substance made to look like a penis.\n",
      "----------------------------------------\n",
      "The room has two unmade beds in it. The beds share a headboard that appears to have an unidentifiable mark or scratches. Both beds have white sheets and a pillow. The bed on the left has a red and tan comforter. The white wall behind the beds has no decoration or hanging picture on it.\n",
      "----------------------------------------\n",
      "The bedroom consists of two twin beds, lying next to each other. Made of wooden material, the beds contain three pillows each and there is a small dresser lying between them. A telephone and double-head lamp is lying on the table. On the other side of the room, there are two chairs lying between a table. They're arranged behind a curtain.\n",
      "========================================\n",
      "Four toy characters sitting next to some pizza slices.\n",
      "----------------------------------------\n",
      "The image is of plastic figurines on a chopping block surrounded by a partially cut pizza. The pizza has cheese and pepperoni on it.  The figures are small and have bunny ears.  The central figure is holding a stick and has a grass hat around it's ears.  Two figures next to it are of a male and a female facing towards each other and the one on the other side faces the pizza edge.\n",
      "----------------------------------------\n",
      "A closeup of a pizza on a plate.The background is out of focus. The pizza is an \"artisan\" style pizza, that you would find in an upscale restaurant. The visible ingredients are cheese, basil leaves, spinach, and whole tomato slices. The crust is white and slightly browned. The pizza rests on a white platter.\n",
      "========================================\n",
      "A white jet sitting inside of a hangar next to other aircraft.\n",
      "----------------------------------------\n",
      "Several white airplane are indoors. An American flag hung from a yellow structure. The American flag is next to a white airplane. The ceiling of the building has glass windows. There is a number on the wing of the airplane. The floor of the building is white. Three ropes hung from a yellow structure. The wing of white plane is blue and red.\n",
      "----------------------------------------\n",
      "A woman in a colorful shirt is standing in front of an airplane. The airplane is white, blue and red. There is a truck next to the plane. The woman is holding an umbrella. \n",
      "========================================\n",
      "Woman with microphone standing in a room full of people eating.\n",
      "----------------------------------------\n",
      "A woman wearing a black sleeveless dress is standing in a room filled with people. The people are sitting at tables and clapping their hands. The tables are covered with white tablecloths. There are plates and wine glasses sitting on top of the tables. The walls of the room have illuminated purple lights on them.\n",
      "----------------------------------------\n",
      "\n",
      "There are three people standing around facing each other. There is a man wearing a blue vest over a shirt. A woman is facing the man. She has a long blond ponytail. She is holding a silver white purse under her left arm. A third person can be partially seen. They are holding a wine glass and wearing a blue wristband. The man is holding a glass of wine and is talking. They are standing in front of a table with a white tablecloth. The table has papers on it. \n",
      "========================================\n",
      "A little boy is at a dining table in public.\n",
      "----------------------------------------\n",
      "A kid in a black shirt with white sleeves is sitting. There is a womans hand in his face the woman is wearing a ring there are ketchup and mustard bottles on the table and a group of people sitting down eating in the background. The walls of the place are red and the floor is black. There is also a white napkin on the table. There are silverware on top of the napkins as well.\n",
      "----------------------------------------\n",
      "two children are sitting at a table in a restaurant.  the children are one little girl and one little boy.  The little girl is eating a pink frosted donut with white icing lines on top of it .  The girl has blonde hair and is wearing a green jacket with a black long sleeve shirt underneath.  the little boy is wearing a black zip up jacket and is holding his finger to his lip but is not eating.  A metal napkin dispenser is in between them at the table.  the wall next to them is white brick.  two adults are on the other side of the short white brick wall.  the room has white circular lights on the ceiling and a large window in the front of the restaurant.  it is daylight outside.\n"
     ]
    }
   ],
   "source": [
    "# caption, paragraph for same image, paragraph for different by similar image\n",
    "n = 5\n",
    "\n",
    "available_iis_cappar = df['cocoparcapdf']['image_id_x']\n",
    "available_iis_sim = coco_id2semsim.keys()\n",
    "available_iis = set(available_iis_cappar).intersection(available_iis_sim)\n",
    "# len(available_iis)    # Only 1503 available...\n",
    "\n",
    "for _ in range(n):\n",
    "    ii = np.random.choice(list(available_iis))\n",
    "    cap, ppar = df['cocoparcapdf'][df['cocoparcapdf']['image_id_x'] == ii]['caption paragraph'.split()].values[0]\n",
    "    all_sim = n_most_sim(coco_sem_sim, coco_sem_map, coco_id2semsim[ii], n=200)\n",
    "    all_neg = set(available_iis).intersection(all_sim)\n",
    "    nii = np.random.choice(list(all_neg))\n",
    "    npar = df['cocoparcapdf'][df['cocoparcapdf']['image_id_x'] == nii]['paragraph'].values[0]\n",
    "\n",
    "    print('=' * 40)\n",
    "    print(cap)\n",
    "    print('-' * 40)\n",
    "    print(ppar)\n",
    "    print('-' * 40)\n",
    "    print(npar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... between Discursive Scene Descriptions and Follow-Ups\n",
    "\n",
    "Following the same recipe, we can create other, related tasks as well, such as the following: \"Given a sequence of region descriptions from one scene, predict whether an additional single region description comes from the same scene or not.\" \n",
    "\n",
    "This is a variant of the Caption / There is task from above, except that here the premise is not assumed to fully describe the base situation; the particular relation that links the hypothesis to the premise could be called \"continuation\" or \"thematic coherence\". \n",
    "\n",
    "First, with randomly selected distractor scene for the wrong hypothesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "The scene:\n",
      "  seagulls flocked together at the shore\n",
      "  lake is frozen over\n",
      "  house around the reservior\n",
      "  wet sand at a beach\n",
      "  animal in the water\n",
      "  the dog is black\n",
      "  a dog on the sand\n",
      "  person standing on beach in the distance\n",
      "  dark blue patches on icy water\n",
      "Which of the following belongs to the same scene?\n",
      " A: a person casting a shadow on the sand\n",
      " B: Patch of bright green grass\n",
      "========================================\n",
      "The scene:\n",
      "  an arched entryway to the church\n",
      "  red roof on building\n",
      "  Clock on the side of the building\n",
      "  a path going to the building\n",
      "  Arched black and yellow door\n",
      "  red and black shingle slanted roof\n",
      "  Window on the building\n",
      "  bright blue sky with white clouds\n",
      "  Gravel pathway through green grass\n",
      "Which of the following belongs to the same scene?\n",
      " A: Wooden door way\n",
      " B: Black tire of a car\n",
      "========================================\n",
      "The scene:\n",
      "  The chairs are green.\n",
      "  man at a cafe\n",
      "  green chairs and table outside\n",
      "  he is wearing tan pants.\n",
      "  windows in front of a restaurant\n",
      "  green plastic patio tables\n",
      "  reflection of the street\n",
      "  name inscribed in the wood\n",
      "  He is leaning.\n",
      "Which of the following belongs to the same scene?\n",
      " A: a man and a street sign reflected\n",
      " B: the giraffe is brown and white in color\n"
     ]
    }
   ],
   "source": [
    "# deep caption + follow up: plausible or not? Randomly selected neg hyp.\n",
    "\n",
    "n_egs = 3\n",
    "\n",
    "for _ in range(n_egs):\n",
    "    ic, ii = df['vgregdf'].sample()[['i_corpus', 'image_id']].values[0]\n",
    "\n",
    "    prem_set_all = list(set(query_by_id(df['vgregdf'], (ic, ii), 'phrase')))\n",
    "    prem_set = np.random.choice(prem_set_all, min(10, len(prem_set_all)), replace=False)\n",
    "    np.random.shuffle(prem_set)\n",
    "    phyp = prem_set[-1]\n",
    "    prem_set = prem_set[:-1]\n",
    "\n",
    "    nii = df['vgregdf'].sample()['image_id'].values[0]\n",
    "    nhyp = np.random.choice(query_by_id(df['vgregdf'], (ic, nii), 'phrase'))\n",
    "\n",
    "    print(\"=\" * 40)\n",
    "    print(\"The scene:\")\n",
    "    for rg in prem_set:\n",
    "        print(' ', rg)\n",
    "    print(\"Which of the following belongs to the same scene?\")\n",
    "    print(\" A:\", phyp)\n",
    "    print(\" B:\", nhyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the distractor scene selected for similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "The scene:\n",
      "  vegetables inside of soup\n",
      "  brown of toasted bread\n",
      "  Golden brown toast\n",
      "  green lettuce of sandwich\n",
      "  a piece of an orange\n",
      "  Dark soup in bowl\n",
      "  a piece of a sandwich\n",
      "  edge of a soup spoon\n",
      "  soup in a bowl on a plate\n",
      "Which of the following belongs to the same scene?\n",
      " A: surface of gray marble\n",
      " B: Food on a tray\n",
      "========================================\n",
      "The scene:\n",
      "  colorful plant behind gate\n",
      "  A little yellow train.\n",
      "  the sky is white.\n",
      "  yellow and black front of train\n",
      "  large rosebush behind the fence\n",
      "  the grass is green.\n",
      "  A building in the distance.\n",
      "  small yellow and red train for tourists\n",
      "  the sign is green.\n",
      "Which of the following belongs to the same scene?\n",
      " A: train is yellow and black\n",
      " B: Backhoe construction equipment\n",
      "========================================\n",
      "The scene:\n",
      "  roof of building with columns\n",
      "  small buildings attached to railings\n",
      "  entrance to the building\n",
      "  a couple of lights\n",
      "  steps leading to door\n",
      "  solar panels on roof\n",
      "  leafless tree on the grass\n",
      "  entryway to the building with columns\n",
      "  multi floored building between structures\n",
      "Which of the following belongs to the same scene?\n",
      " A: the lawn is green \n",
      " B: A clock on the tower.\n"
     ]
    }
   ],
   "source": [
    "# deep caption + follow up: plausible or not? Neg hyp from similar image.\n",
    "ic = icorpus_code['visual_genome']\n",
    "reg_sim_iis = list(set(df['vgregdf']['image_id']).intersection(set(visg_id2semsim.keys())))\n",
    "\n",
    "n_egs = 3\n",
    "\n",
    "for _ in range(n_egs):\n",
    "    \n",
    "    ii = np.random.choice(reg_sim_iis)\n",
    "    # ic, ii = df['vgregdf'].sample()[['i_corpus', 'image_id']].values[0]\n",
    "\n",
    "    prem_set_all = list(set(query_by_id(df['vgregdf'], (ic, ii), 'phrase')))\n",
    "    prem_set = np.random.choice(prem_set_all, min(10, len(prem_set_all)), replace=False)\n",
    "    np.random.shuffle(prem_set)\n",
    "    phyp = prem_set[-1]\n",
    "    prem_set = prem_set[:-1]\n",
    "\n",
    "    nm = n_most_sim(visg_sem_sim, visg_sem_map, visg_id2semsim[ii], n=100)\n",
    "    #nm = n_most_sim(visg_vis_sim, visg_vis_map, visg_id2vissim[ii], n=100)\n",
    "    nm = set(df['vgregdf']['image_id']).intersection(set(nm))\n",
    "    nii = np.random.choice(list(nm))\n",
    "    nhyp = np.random.choice(query_by_id(df['vgregdf'], (ic, nii), 'phrase'))\n",
    "\n",
    "    print(\"=\" * 40)\n",
    "    print(\"The scene:\")\n",
    "    for rg in prem_set:\n",
    "        print(' ', rg)\n",
    "    print(\"Which of the following belongs to the same scene?\")\n",
    "    print(\" A:\", phyp)\n",
    "    print(\" B:\", nhyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarise this section, the general recipe here is to use the external grounding of the expressions (in images) to construct pairings of expressions that are semantically closely related, with the second part of the pair in some sense following from the first. (Or not, for which case the fact that the expressions come from different images is utilised.)\n",
    "\n",
    "* **Dataset:** pairs of expressions related via the same image\n",
    "* **Negative Instances:** expressions taken from other images\n",
    "* **Source:** visual genome, COCO; derived\n",
    "* **Uses:** learn to predict whether given pair is semantically related or not; learn *(common sense) entailment* relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Would a Model have to Learn, and What Might it Look Like?\n",
    "\n",
    "Having looked at various data sets that can be created, which all bring out different aspects of the general *implies* relation, we can pause briefly to ask what a model that learns this relation from that data would have to learn.\n",
    "\n",
    "This first thing to note here is that it seems that it can't just be logical rules (or rather, the meaning of logical constants) that is to be learned here. This might be enough for pairs such as \"all girls are coding / the girl on the left is coding\", but as the expressions used here come from actual use contexts (albeit in annotations), such textbook examples are unlikely to occur. In many of the cases, at the very least additional *linguistic* knowledge is required (e.g., to relate \"a woman is reading\" and \"a person is reading\"). But beyond that, in many of the examples it seems to be knowledge that presumably goes beyond linguistic knowledge and into the common sense domain to recognise the relation (e.g., to know that in a situation described as \"The skate boarder is riding the skateboard down a slope.\", it is likely that \"there is a helmet\" is also true).\n",
    "\n",
    "A straightforward modern approach now would be to use a high-capacity model (most likely a neural network) to train a classifier that takes a pair of expressions and  predicts whether the relation holds or not. (And which in that sense learns and defines the relation.) This is indeed the approach typically taken to the \"Natural Language Inference\" task \\cite{snli:emnlp2015}, and with some good success.\n",
    "\n",
    "We just note here that in the settings described here, other approaches also seem possible. We said above that the semantic view on the relation is that it holds in case every model of the premise is also a model of the hypothesis. Given a way to evaluate an expression relative to a model, as sketched in Section [Expressions and Denotations](#Expressions-and-Denotations), this quantification over models could indeed be realised, as quantification over all available images.\n",
    "\n",
    "This would, however, require the availability of a reasonably large set of reference models -- which perhaps means that the approach loses in cognitive plausiblity. (If that is a goal.) An approach that sits somewhere between the direct prediction and the model exemplar checking would be one where a (set of) models is *predicted* from the premise, against which the hypothesis is then evaluated. There are methods in the literature that aim to go from natural language expressions (typically, captions) to images or semi-symbolic representations such as image layouts \\cite{Hong2018}, \\cite{Zhao2018}, which could perhaps be used for this. The advantage of such a model would be that it would be inspectable and hence lending itself to also making *explanations* derivable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[<a id=\"cit-barwiseperry:sitatt\" href=\"#call-barwiseperry:sitatt\">1</a>] Jon Barwise and John Perry, ``_Situations and Attitudes_'',  1983.\n",
    "\n",
    "[<a id=\"cit-chierchi:meaning\" href=\"#call-chierchi:meaning\">2</a>] Gennaro Chierchia and Sally McConnell-Ginet, ``_Meaning and Grammar: An Introduction to Semantics_'',  1990.\n",
    "\n",
    "[<a id=\"cit-Dagan:rte\" href=\"#call-Dagan:rte\">3</a>] I. Dagan, O. Glickman and B. Magnini, ``_The PASCAL Recognising Textual Entailment Challenge_'', Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment,  2006.  [online](http://dx.doi.org/10.1007/11736790_9)\n",
    "\n",
    "[<a id=\"cit-snli:emnlp2015\" href=\"#call-snli:emnlp2015\">4</a>] S.R. Bowman, G. Angeli, C. Potts <em>et al.</em>, ``_A large annotated corpus for learning natural language inference_'', Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP),  2015.\n",
    "\n",
    "[<a id=\"cit-schlangen:iwcs19\" href=\"#call-schlangen:iwcs19\">5</a>] D. Schlangen, ``_Natural Language Semantics With Pictures: Some Language & Vision Datasets and Potential Uses for Computational Semantics_'', Proceedings of the International Conference on Computational Semantics (IWCS), May 2019.\n",
    "\n",
    "[<a id=\"cit-youngetal:flickr30k\" href=\"#call-youngetal:flickr30k\">6</a>] Young Peter, Lai Alice, Hodosh Micah <em>et al.</em>, ``_From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions_'', Transactions of the Association for Computational Linguistics, vol. 2, number , pp. ,  2014.\n",
    "\n",
    "[<a id=\"cit-turney-pantel:10\" href=\"#call-turney-pantel:10\">7</a>] D. Peter and Pantel Patrick, ``_From Frequency to Meaning: Vector Space Models of Semantics_'', Journal of Artificial Intelligence Research, vol. 37, number , pp. 141--188,  2010.\n",
    "\n",
    "[<a id=\"cit-Mikolov2013:embeddings\" href=\"#call-Mikolov2013:embeddings\">8</a>] T. Mikolov, K. Chen, G. Corrado <em>et al.</em>, ``_Distributed Representations of Words and Phrases and their Compositionality_'', Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013 (NIPS 2013),  2013.\n",
    "\n",
    "[<a id=\"cit-zaschla:contground\" href=\"#call-zaschla:contground\">9</a>] S. Zarrie and D. Schlangen, ``_Deriving continous grounded meaning representations from referentially structured multimodal contexts_'', Proceedings of EMNLP 2017 -- Short Papers, September 2017.\n",
    "\n",
    "[<a id=\"cit-Hong2018\" href=\"#call-Hong2018\">10</a>] Hong Seunghoon, Yang Dingdong, Choi Jongwook <em>et al.</em>, ``_Inferring Semantic Layout for Hierarchical Text-to-Image Synthesis_'', ArXiv, vol. , number Figure 1, pp. ,  2018.  [online](http://arxiv.org/abs/1801.05091)\n",
    "\n",
    "[<a id=\"cit-Zhao2018\" href=\"#call-Zhao2018\">11</a>] Zhao Bo, Meng Lili, Yin Weidong <em>et al.</em>, ``_Image Generation from Layout_'', ArXiv, vol. , number , pp. ,  2018.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "author": "Natural Language Semantics with Pictures: Some Language \\&amp; Vision Datasets and Potential Uses for Computational Semantics",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "../Common/joint.bib",
   "cite_by": "number",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "alt-ctrl-e",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "274px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
